{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"IgWoArVf77sF"},"outputs":[],"source":["import numpy as np\n","from scipy.spatial import distance\n","import matplotlib.pyplot as plt\n","from scipy.spatial.distance import cityblock\n","\n","\n","from transformers import BertModel, BertTokenizer\n","import torch\n"]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased' )"],"metadata":{"id":"X3Ip1drs8CO0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def getEmbedding(text):\n","    tokens = tokenizer(text, return_tensors='pt')\n","    outputs = model(**tokens) # Transformer layers\n","    last_hidden_states = outputs.last_hidden_state\n","    sentence_embedding = torch.mean(last_hidden_states, dim=1)\n","    embedding_array = sentence_embedding.detach().numpy()\n","    return embedding_array\n","\n","# modify getEmbedding array so that it returns the embedding of a specific word within the sentence, parameters are the text and the index of the word\n","# Return the token split of the text as\n","def getEmbeddingWord(text, index):\n","    tokens = tokenizer(text, return_tensors='pt')\n","    outputs = model(**tokens) # Transformer layers\n","    last_hidden_states = outputs.last_hidden_state\n","    word_embedding = last_hidden_states[0][index]\n","    embedding_array = word_embedding.detach().numpy()\n","    return embedding_array\n","\n","# Define a method that takes in text input and shows the tokenized version of the text\n","def tokenizeText(text):\n","    tokens = tokenizer(text, return_tensors='pt')\n","    return tokens\n","\n","# Test method to make sure the input id is correct, takes in an input id and returns the word\n","def getWordFromID(id):\n","    return tokenizer.convert_ids_to_tokens([id])[0]\n","## Reminder, 101 and 102 are the start and end tokens, 0 is the padding token\n","\n","\n"],"metadata":{"id":"H7_Uhhgd8D2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Result 1\n","\n","# Vector Examples\n","vec1 = [\"dog\", \"cat\", \"turtle\"]\n","\n","# Getting the embeddings for the words in vec1\n","vecEmbeds1 = []\n","for vec in vec1:\n","    vecEmbeds1.append(getEmbedding(vec)) # Gets the vector embeddings\n","\n","vec2 = [\"dog\", \"cat\", \"turtle\"]\n","\n","vec2Embeds = []\n","for vec in vec2:\n","    vec2Embeds.append(getEmbedding(vec))\n","\n","\n","cosineMatrix = np.zeros((len(vecEmbeds1), len(vec2Embeds)))\n","\n","for i in range(len(vecEmbeds1)):\n","    for j in range(len(vec2Embeds)):\n","        cosineMatrix[i][j] = 1 - distance.cosine(vecEmbeds1[i], vec2Embeds[j]) # Calculating Cosine Similarity\n","\n","\n","angleMatrix = np.arccos(cosineMatrix) * 180 / np.pi\n","\n","# Plotting Results in Heatmap\n","plt.imshow(angleMatrix, cmap='autumn')\n","\n","# Add cell values as text annotations\n","for i in range(len(vecEmbeds1)):\n","    for j in range(len(vec2Embeds)):\n","        plt.annotate(f'{angleMatrix[i][j]:.2f}', xy=(j, i), ha='center', va='center')\n","\n","plt.title(\"Distance between vectors\")\n","plt.xticks(np.arange(len(vec2)), vec2)\n","plt.yticks(np.arange(len(vec1)), vec1)\n","plt.tick_params(axis='x', labeltop=True, labelbottom=False)\n","plt.colorbar()\n"],"metadata":{"id":"tzpFlJiR8EZA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Question 2\n","### Creating word list and emebdding vectors ###\n","vec1 = [\"Is a dog a mammal?\", \"Is a cat a mammal?\", \"Is a turtle a mammal?\"]\n","\n","vecEmbeds1 = []\n","for vec in vec1:\n","    vecEmbeds1.append(getEmbedding(vec))\n","\n","vec2 = [\"Is a dog a mammal?\", \"Is a cat a mammal?\", \"Is a turtle a mammal?\"]\n","\n","vec2Embeds = []\n","for vec in vec2:\n","    vec2Embeds.append(getEmbedding(vec))\n","\n","### Creating Cosine Distance Matrix ###\n","cosineMatrix = np.zeros((len(vecEmbeds1), len(vec2Embeds)))\n","for i in range(len(vecEmbeds1)):\n","    for j in range(len(vec2Embeds)):\n","        cosineMatrix[i][j] = 1 - distance.cosine(vecEmbeds1[i], vec2Embeds[j])\n","\n","angleMatrix = np.arccos(cosineMatrix) * 180 / np.pi\n","\n","\n","### Plotting Results in Heatmap ###\n","plt.imshow(angleMatrix, cmap='autumn')\n","plt.title(\"Distance between vectors\")\n","\n","for i in range(len(vecEmbeds1)):\n","    for j in range(len(vec2Embeds)):\n","        plt.annotate(f'{angleMatrix[i][j]:.2f}', xy=(j, i), ha='center', va='center')\n","\n","labels = [\"Is a dog\"\"\\n\"\"a mammal?\", \"Is a cat\"\"\\n\"\"a mammal?\", \"Is a turtle\"\"\\n\"\"a mammal?\"] #Slight change to make labels more visible\n","\n","plt.xticks(np.arange(len(vec2)), labels)\n","plt.yticks(np.arange(len(vec1)), vec1)\n","plt.tick_params(axis='x', labeltop=True, labelbottom=False)\n","plt.colorbar()\n"],"metadata":{"id":"_2lsZfUZ8IdF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Bow\n","\n","## Words with multiple meanings: bow\n","vec1 = [\"The bow was damaged.\", \"She has a bow.\", \"He took a bow.\", \"He used a bow\"]\n","vecEmbeds1 = []\n","for vec in vec1:\n","    vecEmbeds1.append(getEmbedding(vec))\n","\n","vecEmbeds2 = []\n","# append bows from each sentence\n","vecEmbeds2.append(getEmbeddingWord(\"The bow was damaged.\", 2))\n","vecEmbeds2.append(getEmbeddingWord(\"She wears a bow.\", 3))\n","vecEmbeds2.append(getEmbeddingWord(\"He took a bow.\", 3))\n","vecEmbeds2.append(getEmbeddingWord(\"He used a bow\", 3))\n","\n","\n","cosineMatrix = np.zeros((len(vecEmbeds1), len(vecEmbeds2)))\n","for i in range(len(vecEmbeds1)):\n","    for j in range(len(vecEmbeds2)):\n","        cosineMatrix[i][j] = 1 - distance.cosine(vecEmbeds1[i], vecEmbeds2[j])\n","\n","angleMatrix = np.arccos(cosineMatrix) * 180 / np.pi\n","\n","plt.imshow(angleMatrix, cmap='autumn')\n","\n","vec2 = [\"bow -\"\"\\n\"\" prompt 1\", \"bow -\" \"\\n\" \"prompt 2\", \"bow -\"\"\\n\" \"prompt 3\",\"bow -\"\"\\n\" \"prompt 4\"]\n","\n","vec1 = [\"1. The bow was\" \"\\n\"\"damaged.\", \"2. She wears a bow.\", \"3. He took a bow.\", \"4. He used a bow\"]\n","\n","plt.title(\"Distance between vectors\")\n","plt.xticks(np.arange(len(vec2)), vec2)\n","plt.yticks(np.arange(len(vec1)), vec1)\n","plt.tick_params(axis='x', labeltop=True, labelbottom=False)\n","plt.colorbar()\n","\n","# add angle values to the heatmap\n","for i in range(len(vecEmbeds1)):\n","    for j in range(len(vecEmbeds2)):\n","        plt.annotate(f'{angleMatrix[i][j]:.2f}', xy=(j, i), ha='center', va='center')"],"metadata":{"id":"OMPHEcUHfWnJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Words with multiple meanings: bow\n","vec1 = [\"Can you take a bow?\", \"Can you take a rainbow?\"]\n","vecEmbeds1 = []\n","for vec in vec1:\n","    vecEmbeds1.append(getEmbedding(vec))\n","\n","vecEmbeds2 = []\n","vecEmbeds2.append(getEmbeddingWord(\"Can you take a bow?\", 5))\n","vecEmbeds2.append(getEmbeddingWord(\"Can you take a rainbow?\", 5))\n","\n","cosineMatrix = np.zeros((len(vecEmbeds1), len(vecEmbeds2)))\n","for i in range(len(vecEmbeds1)):\n","    for j in range(len(vecEmbeds2)):\n","        cosineMatrix[i][j] = 1 - distance.cosine(vecEmbeds1[i], vecEmbeds2[j])\n","\n","angleMatrix = np.arccos(cosineMatrix) * 180 / np.pi\n","\n","plt.imshow(angleMatrix, cmap='autumn')\n","\n","#vec2 = [\"bow - ship\", \"bow - hair\", \"bow -\"\"\\n\" \"perform\",\"bow -\"\"\\n\" \"arrow\" ,\"bow -\" \"\\n\" \"no context\"]\n","vec2 = [\"bow - bow\", \"bow - rainbow\"]\n","\n","plt.title(\"Distance between vectors\")\n","plt.xticks(np.arange(len(vec2)), vec2)\n","plt.yticks(np.arange(len(vec1)), vec1)\n","plt.tick_params(axis='x', labeltop=True, labelbottom=False)\n","plt.colorbar()\n","\n","# add angle values to the heatmap\n","for i in range(len(vecEmbeds1)):\n","    for j in range(len(vecEmbeds2)):\n","        plt.annotate(f'{angleMatrix[i][j]:.2f}', xy=(j, i), ha='center', va='center')"],"metadata":{"id":"_fhtxrkxfc0o"},"execution_count":null,"outputs":[]}]}